{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Comparison of Galaxy Cluster Centering Models\n",
    "\n",
    "In the [previous tutorial](model_evaluation.ipynb), we evaluated the \"goodness of fit\" of two different models on a data set representing the distribution of centering offsets in a galaxy cluster sample. Here we will continue by doing two styles of quantitative test to compare the two models, with the goal of deciding whether an improved goodness of fit actually justifies the additional complexity of the second model. Specifically, you will calculate\n",
    "* the Deviance Information Criterion, one of several possible information criteria, which has the advantages of being relatively simple and having a straightforward Bayesian interpretation;\n",
    "* the Bayesian evidence, a more principled but more complex approach fully in the Bayesian framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TutorialName = 'model_comparison'\n",
    "exec(open('tbc.py').read()) # define TBC and TBC_above\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "from scipy.special import logsumexp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import dynesty\n",
    "from dynesty import plotting as dyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting set up\n",
    "\n",
    "First, we'll want to take advantage of your work in the previous tutorial. As irritating as it may be, paste the (completed) definitions of `Model`, `ExponentialModel` and your alternative model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is something we can throw to discourage direct instantiation of the base class\n",
    "class VirtualClassError(Exception):\n",
    "    def __init__(self):\n",
    "        Exception.__init__(self,\"Do not directly instantiate the base Model class!\")\n",
    "\n",
    "class Model:\n",
    "    \"\"\"\n",
    "    Base class for inference and model evaluation in a simple cluster mis-centering analysis.\n",
    "    In all these functions, `params' is a dictionary of model parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, samples=None):\n",
    "        \"\"\"\n",
    "        Note: derived classes should have their own __init__ function which ends by calling this one\n",
    "        \"\"\"\n",
    "        # Storage for MCMC samples from fitting the model\n",
    "        self.samples = samples\n",
    "        if samples is None:\n",
    "            self.Nsamples = 0\n",
    "        else:\n",
    "            self.Nsamples = samples.shape[0]\n",
    "        # Lines below are meant as a template for derived classes, do not uncomment for the base class\n",
    "        #\n",
    "        # A name for the model!\n",
    "        #self.name = '...'\n",
    "        #\n",
    "        # As part of the model definition, store an ordered list of parameter names (for dictionary keys)\n",
    "        # Named arguments of other functions that correspond to parameters should appear in this order\n",
    "        #self.param_names = [...]\n",
    "        #\n",
    "        # parameter names as display text (for plot labels),\n",
    "        #self.param_labels = [...]\n",
    "        #\n",
    "        # and a dictionary of priors (keys must match param_names)\n",
    "        #   (we like to use frozen scipy.stats distribution objects for this, but you'll be implementing the log_prior\n",
    "        #    function below, so really the dictionary entry could be anything that specifies the prior)\n",
    "        #self.priors = {...}\n",
    "        #\n",
    "        # The next line finishes initialization by calling the parent class' __init__\n",
    "        #Model.__init__(self, samples)\n",
    "  \n",
    "    def log_prior(self, **params):\n",
    "        \"\"\"\n",
    "        Return the log prior PDF p(params|H)\n",
    "        \"\"\"\n",
    "        raise VirtualClassError # to be overriden by child classes\n",
    "\n",
    "    def draw_samples_from_prior(self, N):\n",
    "        \"\"\"\n",
    "        Return N samples from the prior PDF p(params|H) as a list of dictionaries\n",
    "        \"\"\"\n",
    "        raise VirtualClassError # to be overriden by child classes\n",
    "\n",
    "    def log_likelihood(self, **params):\n",
    "        \"\"\"\n",
    "        Return the log of the likelihood function L(params) = p(y|params,H)\n",
    "        \"\"\"\n",
    "        raise VirtualClassError # to be overriden by child classes\n",
    "    \n",
    "    def sampling_distribution(self, yy, **params):\n",
    "        \"\"\"\n",
    "        Return the sampling distribution p(yy|params,H) at a point in data space yy given parameter(s) args\n",
    "        We expect a vector input yy, and return the corresponding probabilities.\n",
    "            \n",
    "        Note: This is useful for making plots of \"the model\" overlaid on the histogram of the data\n",
    "        \"\"\"\n",
    "        raise VirtualClassError # to be overriden by child classes \n",
    "        \n",
    "    def generate_replica_dataset(self, N, **params):\n",
    "        \"\"\"\n",
    "        Return a replica dataset y_rep of length N from the sampling distribution p(y_rep|args,H).\n",
    "        \"\"\"\n",
    "        raise VirtualClassError # to be overriden by child classes\n",
    "        \n",
    "    def log_posterior(self, parameterlist=None, **params):\n",
    "        \"\"\"\n",
    "        Return the log of the (unnormalized) posterior PDF p(params|y,H)\n",
    "        \n",
    "        The parameterlist argument is there for compatibility with emcee.\n",
    "        This will be an infinite loop if you named one of your parameters \"parameterlist\", but in that case\n",
    "        you deserve it.\n",
    "        This also means that for even a simple call, we would need to specify the parameter name(s),\n",
    "         i.e. not log_posterior(pval), but log_posterior(param=pval). This doesn't seem unreasonable.\n",
    "        \"\"\"\n",
    "        if parameterlist is not None:\n",
    "            pdict = {k:parameterlist[i] for i,k in enumerate(self.param_names)}\n",
    "            return self.log_posterior(**pdict)\n",
    "        lnp = self.log_prior(**params)\n",
    "        print(lnp)\n",
    "        if lnp != -np.inf:\n",
    "            lnp += self.log_likelihood(**params)\n",
    "        return lnp\n",
    "\n",
    "    def draw_samples_from_posterior(self, starting_params, nsteps, nwalkers=8, threads=1):\n",
    "        \"\"\"\n",
    "        Use emcee to draw samples from P(params|y,H). Hopefully it Just Works.\n",
    "        You could try using e.g. threads=4 to speed things up with multiprocessing.\n",
    "        \"\"\"\n",
    "      \n",
    "        # The density to sample is this model's own posterior PDF\n",
    "        npars = len(starting_params)        \n",
    "        self.sampler = emcee.EnsembleSampler(nwalkers, npars, self.log_posterior, threads=threads) \n",
    "        \n",
    "        # Generate an ensemble of walkers within +/-1% of the guess:\n",
    "        theta_0 = np.array([starting_params*(1.0 + 0.01*np.random.randn(npars)) for j in range(nwalkers)])\n",
    "        # Note that the initial parameter array theta_0 should have dimensions nwalkers x npars\n",
    "        \n",
    "        # Evolve the ensemble:\n",
    "        self.sampler.run_mcmc(theta_0, nsteps)\n",
    "        \n",
    "        # Plot the raw samples:\n",
    "        plt.rcParams['figure.figsize'] = (12.0, 4.0*npars)\n",
    "        fig, ax = plt.subplots(npars, 1);\n",
    "        cr.plot_traces(self.sampler.chain[:min(8,nwalkers),:,:], ax, labels=self.param_labels);\n",
    "\n",
    "    def check_chains(self, burn, maxlag):\n",
    "        '''\n",
    "        Ignoring `burn` samples from the front of each chain, compute convergence criteria and\n",
    "        effective number of samples.\n",
    "        '''\n",
    "        nwalk, nsteps, npars = self.sampler.chain.shape\n",
    "        if burn < 1 or burn >= nsteps:\n",
    "            return\n",
    "        tmp_samples = [self.sampler.chain[i,burn:,:] for i in range(nwalk)]\n",
    "        print('R =', cr.GelmanRubinR(tmp_samples))\n",
    "        print('neff =', cr.effective_samples(tmp_samples, maxlag=maxlag))\n",
    "        print('NB: Since walkers are not independent, these will be optimistic!')\n",
    "\n",
    "    def remove_burnin(self, burn):\n",
    "        '''\n",
    "        Remove `burn` samples from the front of each chain, and concatenate\n",
    "        Plot, and store the result in self.samples\n",
    "        '''\n",
    "        nwalk, nsteps, npars = self.sampler.chain.shape\n",
    "        if burn < 1 or burn >= nsteps:\n",
    "            return\n",
    "        self.samples = self.sampler.chain[:,burn:,:].reshape(nwalk*(nsteps-burn), npars)\n",
    "        self.Nsamples = self.samples.shape[0]\n",
    "        plt.rcParams['figure.figsize'] = (12.0, 4.0*npars)\n",
    "        fig, ax = plt.subplots(npars, 1);\n",
    "        cr.plot_traces(self.samples, ax, labels=self.param_labels);\n",
    "        \n",
    "    def posterior_mean(self):\n",
    "        '''\n",
    "        Helper function for computing the posterior mean of each parameter (from MCMC samples)\n",
    "        '''\n",
    "        m = np.mean(self.samples, axis=0)\n",
    "        return {k:m[i] for i,k in enumerate(self.param_names)}\n",
    "class ExponentialModel(Model):\n",
    "    \"\"\"\n",
    "    Simple exponential model for mis-centering.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # As part of the model definition, store an ordered list of parameter names (for dictionary keys)\n",
    "        # A name for the model!\n",
    "        self.name = 'Exponential Model'\n",
    "        # Named arguments of other functions that correspond to parameters should appear in this order\n",
    "        self.param_names = ['a']\n",
    "        # parameter names as display text (for plot labels),\n",
    "        self.param_labels = [r'$a$']\n",
    "        # and a dictionary of priors (keys must match param_names)\n",
    "        #   (we like to use frozen scipy.stats distribution objects for this, but you'll be implementing the log_prior\n",
    "        #    function below, so really the dictionary entry could be anything that specifies the prior)\n",
    "        self.priors = {'a':st.uniform()}\n",
    "        # The next line finishes initialization by calling the parent class' __init__\n",
    "        Model.__init__(self, *args, **kwargs)\n",
    "  \n",
    "    def log_prior(self, a):\n",
    "            \n",
    "            \"\"\"\n",
    "            Return the log prior PDF P(a1|H)\n",
    "            \"\"\"\n",
    "            rv = st.uniform(loc=0.1,scale=100)\n",
    "            return rv.logpdf(a)\n",
    "\n",
    "    def draw_samples_from_prior(self, N):\n",
    "        \"\"\"\n",
    "        Return N samples of a from the prior PDF p(a|H) as a 2D array\n",
    "        (one column per parameter)\n",
    "        \"\"\"\n",
    "        res=[]\n",
    "        for i in range(N):\n",
    "            a=st.uniform.rvs(loc=0.1,scale=100)\n",
    "            y=st.expon.rvs(a)\n",
    "            res.append([a,y])\n",
    "        return np.array(res)\n",
    "    def log_likelihood(self, a):\n",
    "        \"\"\"\n",
    "        Evaluate the log of the likelihood function L(a) = p(y|a,H)\n",
    "        \"\"\"\n",
    "        return np.sum(st.expon.logpdf(y, scale=a))\n",
    "    \n",
    "    def sampling_distribution(self, yy, a):\n",
    "        \"\"\"\n",
    "        Evaluate the sampling distribution PDF p(yy|a,H) at a point in data space yy given parameter value a\n",
    "        We expect a vector input yy, and return the corresponding probabilities.\n",
    "            \n",
    "        Note: This is useful for making plots of \"the model\" overlaid on the histogram of the data\n",
    "        \"\"\"\n",
    "        return st.expon.pdf(yy, scale=a)\n",
    "        \n",
    "    def generate_replica_dataset(self, N, a):\n",
    "        \"\"\"\n",
    "        Return a replica data set y_rep of length N from the sampling distribution p(y_rep|a,H).\n",
    "        \"\"\"\n",
    "        res=[]\n",
    "        for i in range(N):\n",
    "            y=st.expon(scale=a).rvs()\n",
    "            res.append(y)\n",
    "        return np.array(res)\n",
    "class LomaxModel(Model):\n",
    "    \"\"\"\n",
    "    Simple exponential model for mis-centering.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # As part of the model definition, store an ordered list of parameter names (for dictionary keys)\n",
    "        # A name for the model!\n",
    "        self.name = 'Lomax Model'\n",
    "        # Named arguments of other functions that correspond to parameters should appear in this order\n",
    "        self.param_names = ['a']\n",
    "        # parameter names as display text (for plot labels),\n",
    "        self.param_labels = [r'$a$']\n",
    "        # and a dictionary of priors (keys must match param_names)\n",
    "        #   (we like to use frozen scipy.stats distribution objects for this, but you'll be implementing the log_prior\n",
    "        #    function below, so really the dictionary entry could be anything that specifies the prior)\n",
    "        self.priors = {'a':st.uniform()}\n",
    "        # The next line finishes initialization by calling the parent class' __init__\n",
    "        Model.__init__(self, *args, **kwargs)\n",
    "  \n",
    "    def log_prior(self, a):\n",
    "            \n",
    "            \"\"\"\n",
    "            Return the log prior PDF P(a1|H)\n",
    "            \"\"\"\n",
    "            rv = st.uniform(loc=0.1,scale=140)\n",
    "            return rv.logpdf(a)\n",
    "\n",
    "    def draw_samples_from_prior(self, N):\n",
    "        \"\"\"\n",
    "        Return N samples of a from the prior PDF p(a|H) as a 2D array\n",
    "        (one column per parameter)\n",
    "        \"\"\"\n",
    "        res=[]\n",
    "        for i in range(N):\n",
    "            a=st.uniform.rvs(loc=0.1,scale=100)\n",
    "            y=st.lomax.rvs(a)\n",
    "            res.append([a,y])\n",
    "        return np.array(res)\n",
    "    def log_likelihood(self, a):\n",
    "        \"\"\"\n",
    "        Evaluate the log of the likelihood function L(a) = p(y|a,H)\n",
    "        \"\"\"\n",
    "        return np.sum(st.lomax.logpdf(y, a))\n",
    "    \n",
    "    def sampling_distribution(self, yy, a):\n",
    "        \"\"\"\n",
    "        Evaluate the sampling distribution PDF p(yy|a,H) at a point in data space yy given parameter value a\n",
    "        We expect a vector input yy, and return the corresponding probabilities.\n",
    "            \n",
    "        Note: This is useful for making plots of \"the model\" overlaid on the histogram of the data\n",
    "        \"\"\"\n",
    "        return st.lomax.pdf(yy, a)\n",
    "        \n",
    "    def generate_replica_dataset(self, N, a):\n",
    "        \"\"\"\n",
    "        Return a replica data set y_rep of length N from the sampling distribution p(y_rep|a,H).\n",
    "        \"\"\"\n",
    "        res=[]\n",
    "        for i in range(N):\n",
    "            y=st.lomax(a).rvs()\n",
    "            res.append(y)\n",
    "        return np.array(res)## Instantiate it:\n",
    "Model2 = LomaxModel() # replace with the actual name of your new class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll read in the chains for each model that you produced. (You'll need to fill in the name of your model class.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model1 = ExponentialModel(samples=np.loadtxt('centering_model1_chain.txt', ndmin=2))\n",
    "Model2 = LomaxModel(samples=np.loadtxt('centering_model2_chain.txt', ndmin=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're as lazy as me, your implementation above relied on the data `y` being at global scope, so here they are again. If there were any other global variable dependences I haven't anticipated, you'll need to reproduce their definitions here also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([39.30917,35.13419,5.417072,59.75137,30.69077,14.45971,27.07368,27.48429,80.60219,483.1432,24.65057,\n",
    "              22.36524,43.39081,39.89816,30.67409,6.905061,53.69709,9.504133,41.07874,10.9369,48.29861,61.34125,\n",
    "              68.37279,30.51124,26.74462,13.7165,6.043301,976.1495,27.20097,7.818419,5.589193,3.310114,271.8901,\n",
    "              126.0384,99.51247,249.1279,403.0484,3.071718,0.9434036,54.94336,1.529382,8.441071,19.59434,59.43049,\n",
    "              77.21293,29.6533,286.7116,11.2386,9.511912,29.04711,33.77766,151.4803,223.3557,12.33816,25.22682,\n",
    "              26.86597,339.7084,405.6737,3.809868,221.6523,307.2994,73.36697,42.15523,36.74785,5.415392,69.4721,\n",
    "              136.8073,17.3534,4.135966,20.19435,79.06968,8.095599,4.474533,44.90669,85.891,1.636425,75.39335,\n",
    "              15.94149,2.828709,20.5636,41.52905,42.51133,104.3908,67.41335,13.80204,394.9841,33.90415,84.78714,\n",
    "              36.77924,14.48424,66.01276,2.910331,92.79938,29.74337,42.40971,1.692674,1.039994,120.5902,154.7106,\n",
    "              14.38967,147.8399,166.5054,87.53685,22.63141,638.1976,273.6167,593.4997,45.57279,87.30421,75.03385,\n",
    "              18.33932,36.05779,3.659462,263.9074,0.2432062,8.499095,1.160031,38.16615,41.65371,361.5,148.9294,\n",
    "              10.25777,71.29159,10.02279,16.36062,601.1667,4.960311,12.22526,87.54137,48.48371,78.56777,212.8153,\n",
    "              77.0353,62.7624,81.26739,34.36881,42.63432,264.4551,15.24863,25.94133,35.88882,34.94669,222.5425,\n",
    "              304.9676,19.68377,7.216153,17.61534,32.25887,14.08842,773.5914])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Calculate the DIC for each model\n",
    "\n",
    "Recall that the Deviance Information Criterion is given by:\n",
    "\n",
    "$\\mathrm{DIC} = \\langle D(\\theta) \\rangle + p_D; \\quad p_D = \\langle D(\\theta) \\rangle - D(\\langle\\theta\\rangle)$\n",
    "\n",
    "where $\\theta$ are the parameters of a model, the deviance $D(\\theta)=-2\\log P(\\mathrm{data}|\\theta)$, and averages $\\langle\\rangle$ are over the posterior distribution of $\\theta$.\n",
    "\n",
    "Write a function to compute this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DIC(Model):\n",
    "    \"\"\"\n",
    "    Compute the Deviance Information Criterion for the given model.\n",
    "    (In a less pedagogical world, this would logically be a method of the base Model class.)\n",
    "    \"\"\"\n",
    "    loglikely=np.array([ Model.log_likelihood(*params) for params in Model.samples])\n",
    "    # Compute the deviance D for each sample\n",
    "    D = -2.0*loglikely\n",
    "    theta=np.exp(loglikely)\n",
    "    Dmean=np.sum(D*theta)\n",
    "    Tmean=Model.posterior_mean()['a']\n",
    "    DTmean=-2*Model.log_posterior(a=Tmean)\n",
    "    pD = Dmean - DTmean\n",
    "    DIC = Dmean-pD\n",
    "    return DIC, pD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the DIC for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.605170185988092\n",
      "Exponential Model:\n",
      "Effective number of fitted parameters = -1675.2308250189826\n",
      "DIC = 1675.2308250189826\n"
     ]
    }
   ],
   "source": [
    "DIC1, pD1 = DIC(Model1)\n",
    "print(Model1.name+':')\n",
    "print(\"Effective number of fitted parameters =\", pD1)\n",
    "print(\"DIC =\", DIC1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** For Model 1 (the exponential), I get $p_D \\approx 1.0$ and DIC $\\approx 1668$. As with anything else computed from chains, there will be some stochasticity to the values you compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.941642422609304\n",
      "Lomax Model:\n",
      "Effective number of fitted parameters = -1792.6495889403761\n",
      "DIC = 1792.6495889403761\n"
     ]
    }
   ],
   "source": [
    "DIC2, pD2 = DIC(Model2)\n",
    "print(Model2.name+':')\n",
    "print(\"Effective number of fitted parameters =\", pD2)\n",
    "print(\"DIC =\", DIC2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do your values of $p_D$ make intuitive sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to interpret this, we can compare the reduction (hopefully) in the DIC of Model 2 compared with Model 1 to the Jeffreys scale (see the [notes](../notes/model_evaluation.ipynb)). By this metric, is your second model better at explaining the data than the exponential model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-117.41876392139352"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIC1 - DIC2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YES~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute the evidence by Monte Carlo integration\n",
    "\n",
    "To do this, note that\n",
    "\n",
    "$p(\\mathrm{data}|H)=\\int d\\theta \\, p(\\mathrm{data}|\\theta,H) \\, p(\\theta|H)$\n",
    "\n",
    "can be approximated by averaging the likelihood over samples from the prior:\n",
    "\n",
    "$p(\\mathrm{data}|H) \\approx \\frac{1}{m}\\sum_{k=1}^m p(\\mathrm{data}|\\theta_k,H)$, with $\\theta_k\\sim p(\\theta|H)$.\n",
    "\n",
    "This estimate is much more straightforward than trying to use samples from the posterior to calculate the evidence (which would require us to be able to normalize the posterior, which would require an estimate of the evidence, ...). But in general, and especially for large-dimensional parameter spaces, it is very inefficient (because the likelihood typically is large in only a small fraction of the prior volume). Still, let's give it a try.\n",
    "\n",
    "Write a function to draw a large number of samples from the prior and use them to calculate the evidence. To avoid numerical over/underflows, use the special `scipy` function `logsumexp` (which we imported directly, way at the top of the notebook) to do the sum. As the name implies, this function is equivalent to `log(sum(exp(...)))`, but is more numerically stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "def log_evidence(Model, N=1000):\n",
    "    \"\"\"\n",
    "    Compute the log evidence for the model using N samples from the prior\n",
    "    \"\"\"\n",
    "    prior=st.uniform(loc=0.1,scale=100)\n",
    "    likelies=[Model.log_likelihood(prior.rvs()) for _ in range(N)]\n",
    "    return scipy.special.logsumexp([likelies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8231861665482074"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior=st.uniform(loc=0.1,scale=100)\n",
    "prior.rvs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a quick test to check for NaNs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-843.9777077915735, -4974.623585498281)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_evidence(Model1, N=2), log_evidence(Model2, N=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roughly how precisely do we need to know the log Evidence, to be able to compare models? Run `log_evidence` with different values of `N` (the number of prior samples in the average) to until you're satisfied that you're getting a usefully accurate result for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1:\n",
      "CPU times: user 1.3 ms, sys: 0 ns, total: 1.3 ms\n",
      "Wall time: 1.29 ms\n",
      "From 1 samples, the log-evidence is -906.5339075866195\n",
      "CPU times: user 4.07 ms, sys: 0 ns, total: 4.07 ms\n",
      "Wall time: 3.72 ms\n",
      "From 10 samples, the log-evidence is -831.7888633343237\n",
      "CPU times: user 22.6 ms, sys: 0 ns, total: 22.6 ms\n",
      "Wall time: 21.7 ms\n",
      "From 100 samples, the log-evidence is -830.03786974691\n",
      "CPU times: user 168 ms, sys: 0 ns, total: 168 ms\n",
      "Wall time: 168 ms\n",
      "From 1000 samples, the log-evidence is -828.1257782166895\n",
      "CPU times: user 1.67 s, sys: 0 ns, total: 1.67 s\n",
      "Wall time: 1.67 s\n",
      "From 10000 samples, the log-evidence is -825.7489278953306\n"
     ]
    }
   ],
   "source": [
    "print('Model 1:')\n",
    "for Nevidence in [1, 10, 100, 1000, 10000]:\n",
    "    %time logE1 = log_evidence(Model1, N=Nevidence)\n",
    "    print(\"From\", Nevidence, \"samples, the log-evidence is\", logE1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2:\n",
      "CPU times: user 2.15 ms, sys: 0 ns, total: 2.15 ms\n",
      "Wall time: 1.91 ms\n",
      "From 1 samples, the log-evidence is -27490.484511441406\n",
      "CPU times: user 3.6 ms, sys: 0 ns, total: 3.6 ms\n",
      "Wall time: 3.43 ms\n",
      "From 10 samples, the log-evidence is -9988.126438764812\n",
      "CPU times: user 24.3 ms, sys: 0 ns, total: 24.3 ms\n",
      "Wall time: 23.2 ms\n",
      "From 100 samples, the log-evidence is -1974.0743997861218\n",
      "CPU times: user 184 ms, sys: 3.69 ms, total: 187 ms\n",
      "Wall time: 186 ms\n",
      "From 1000 samples, the log-evidence is -899.3921665838085\n",
      "CPU times: user 1.84 s, sys: 0 ns, total: 1.84 s\n",
      "Wall time: 1.84 s\n",
      "From 10000 samples, the log-evidence is -889.2284684525814\n"
     ]
    }
   ],
   "source": [
    "print('Model 2:')\n",
    "for Nevidence in [1, 10, 100, 1000, 10000]:\n",
    "    %time logE2 = log_evidence(Model2, N=Nevidence)\n",
    "    print(\"From\", Nevidence, \"samples, the log-evidence is\", logE2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have log evidences computed for each model. Now what? We just compare their difference to the Jeffreys scale again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-63.47954055725086"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logE2 - logE1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we might end up with a different conclusion as to the strength of any preference of Model 2, compared with the DIC! The reason is that the evidence explicitly accounts for the information in the prior (which, recall, counts as part of the model definition), while the DIC does this much less directly.\n",
    "\n",
    "We could also be good Bayesians and admit that there should be a prior distribution in model space. For example, maybe I have a very compelling theoretical reason why the offset distribution should be exponential (I don't, but just for example). Then, I might need some extra convincing that an alternative model is required.\n",
    "\n",
    "We would then compute the ratio of the posterior probabilities of the models as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference of log posteriors (H2-H1): -65.67676513458707\n",
      "Ratio of posteriors (H2/H1): 2.9987710724956604e-29\n"
     ]
    }
   ],
   "source": [
    "prior_H1 = 0.9 # or your choice\n",
    "prior_H2 = 1.0 - prior_H1 # assuming only these two options\n",
    "\n",
    "log_post_H1 = np.log(prior_H1) + logE1\n",
    "log_post_H2 = np.log(prior_H2) + logE2\n",
    "\n",
    "print('Difference of log posteriors (H2-H1):', log_post_H2-log_post_H1)\n",
    "print('Ratio of posteriors (H2/H1):', np.exp(log_post_H2-log_post_H1)) # NB this one might over/underflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the fitness of the alternative model you chose, you may find that only an extremely lopsided prior in model space would influence your conclusion.\n",
    "\n",
    "Comment on what you find from the evidence, compared with the DIC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute the evidence with `dynesty`\n",
    "\n",
    "To get some experience with a package that uses nested sampling to compute the evidence, let's repeat Section 2 using `dynesty`.\n",
    "\n",
    "Looking at [the docs](https://dynesty.readthedocs.io/en/latest/crashcourse.html), we first need a function that maps the unit cube onto our prior. That is, the code is doing a substitution like\n",
    "\n",
    "$\\int d\\theta\\,p(\\theta|H)\\,p(\\mathrm{data}|\\theta,H) = \\int_0^1 dF \\,p\\left[\\mathrm{data}|\\theta(F),H\\right]$,\n",
    "\n",
    "where $F=\\int_{-\\infty}^\\theta d\\theta'\\,p(\\theta'|H)$ is the cumulative distribution function of the prior (hence the identity $dF/d\\theta = p(\\theta|H)$ makes the equation above work out).\n",
    "\n",
    "If our priors were uniform, the translation from $F$ to $\\theta$ is a simple translation and rescaling, but for other priors it involves going through the prior distribution's quantile function. Fortunately, we assumed uniform priors for Model 1! However, if you followed the advice in the previous notebook to keep a dictionary of univariate priors in the `Model` object, and those priors are `scipy.stats` distributions, it's relatively easy to handle this slightly more general case using functions provided by `scipy`. Regrdless of how you choose to do it, implement a function that performs this transformation for the 1-parameter Exponential model (and priors) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ptform(u):\n",
    "    '''\n",
    "    Input: a vector in the unit cube, 0 <= u[i] <= 1, that conforms to the prior volume.\n",
    "    Output: a vector in our parameter space.\n",
    "    '''\n",
    "    TBC()\n",
    "    \n",
    "TBC_above()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Do a sanity check - the cell below should return the quartiles of the uniform prior you chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ptform([0.25]), ptform([0.5]), ptform([0.75]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need a log-likelihood function that takes a vector of parameters as input, in contrast to how it's written in the Model class. This could be solved by using the ridiculous construction in the `Model.log_posterior` method, or we could just define:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dyn_log_like(params):\n",
    "    return Model1.log_likelihood(*params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can just go ahead and run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sampler = dynesty.NestedSampler(dyn_log_like, ptform, len(Model1.param_names))\n",
    "sampler.run_nested()\n",
    "results = sampler.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And look at some stuff (see the Dynesty documentation for an explanation of what all this is):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a summary of the run.\n",
    "rfig, raxes = dynesty.plotting.runplot(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Evidence panel appears to be utterly unhelpful in this case (at least when I ran it), so below is a plot of the log-evidence as a function of iteration, discarding the beginning when the accumulated evidence is truly tiny. It should show us converging to a value similar to what we found earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (12.0, 4.0)\n",
    "Ndiscard = 100\n",
    "plt.plot(results['logz'][Ndiscard:]);\n",
    "plt.xlabel(\"Iteration - \"+str(Ndiscard), fontsize=12);\n",
    "plt.ylabel(\"Log Evidence\", fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This extracts the log-evidence at the last iteration, which may or not be the approved method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['logz'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare with what we got from simple monte carlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logE1 - results['logz'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was so easy, let's go ahead and do the same for Model 2. Implement the transformation from the unit cube to your model's parameter space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ptform2(u):\n",
    "    '''\n",
    "    Input: a vector in the unit cube, 0 <= u[i] <= 1, that conforms to the prior volume.\n",
    "    Output: a vector in our parameter space.\n",
    "    '''\n",
    "    TBC()\n",
    "    \n",
    "TBC_above()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run this elegant definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dyn_log_like2(params):\n",
    "    return Model2.log_likelihood(*params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go ahead and run the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sampler = dynesty.NestedSampler(dyn_log_like2, ptform2, len(Model2.param_names))\n",
    "sampler.run_nested()\n",
    "results = sampler.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and produce the same set of plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfig, raxes = dynesty.plotting.runplot(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (12.0, 4.0)\n",
    "Ndiscard = 100\n",
    "plt.plot(results['logz'][Ndiscard:]);\n",
    "plt.xlabel(\"Iteration - \"+str(Ndiscard), fontsize=12);\n",
    "plt.ylabel(\"Log Evidence\", fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, extract the final evidence,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['logz'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and compare it to the SMC result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logE2 - results['logz'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the work you'd already done, that was (I hope) staggeringly easy to set up, even if it did take longer to compute the same answer (I find) in this case. In larger-dimensional parameter spaces, it's easy to imagine this method being far preferable to blindly sampling from the prior. Note that we run `dynesty` out of the box here - there are many options (and some potential failure modes) that you should read about before using it in the real world."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
